# -*- coding: utf-8 -*-
"""Saiket Systems project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqwbiHLHtGEXD9yAb1mTDEYV-12SVTBF
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd # Data manipulation and analysis
import numpy as np # Numerical operations
import matplotlib.pyplot as plt # Plotting and visualization
import seaborn as sns # Statistical data visualization
from sklearn.preprocessing import StandardScaler # Feature scaling
from sklearn.model_selection import train_test_split # Splitting data into training and testing sets
from sklearn.linear_model import LogisticRegression # Logistic Regression model
from sklearn.metrics import accuracy_score # Metric for classification accuracy
from sklearn.ensemble import RandomForestClassifier # Random Forest classifier
from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Ensemble methods
from sklearn.feature_selection import chi2 # Chi-squared test for feature selection
from sklearn.metrics import (
    accuracy_score, # Accuracy classification score
    precision_score, # Precision metric
    recall_score, # Recall metric
    f1_score, # F1-score metric
    roc_auc_score, # ROC AUC score
    classification_report, # Text report showing the main classification metrics
    confusion_matrix # Confusion matrix to evaluate classification accuracy
)

#data Load
data = pd.read_csv("/content/drive/MyDrive/Telco_Customer_Churn_Dataset  (1) (1).csv")
data.head()

#Display concise summary of the DataFrame
data.info()

# Print the number of rows and columns in the DataFrame
print("Number of Rows:", data.shape[0])
print("Number of Columns:", data.shape[1])

#Check for Missing Values
missing_values = data.isnull().sum()# To identify and count missing values
print("Missing Values in Each Column:")
print(missing_values)

# Generate descriptive statistics for numerical columns in the DataFrame
descriptive_stats = data.describe()
print("Descriptive Statistics for Numerical Columns:")
print(descriptive_stats)

# Print unique values for each categorical column in the DataFrame
print("Unique Values for Categorical Columns:")
for col in data.columns:
    if data[col].dtype == 'object':# Check if the column has object dtype (typically categorical)
        print(f"\nColumn '{col}':")
        print(data[col].unique())

# identify and count duplicate rows
duplicate_rows = data.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")

# Remove rows where 'TotalCharges' is null and verify the removal
data.dropna(subset=['TotalCharges'], inplace=True)
print(f"Number of missing values in 'TotalCharges' after removal: {data['TotalCharges'].isnull().sum()}")

# Replace empty strings or strings containing only spaces with NaN
data['TotalCharges'] = data['TotalCharges'].replace(' ', np.nan)
# Drop rows where TotalCharges is now NaN (after replacing problematic strings)
data.dropna(subset=['TotalCharges'], inplace=True)
# Convert to numeric
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'])
data.info()

# List of numerical columns to be scaled
numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

# Instantiate StandardScaler
scaler = StandardScaler()

# Apply fit_transform to the selected numerical columns
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

print("Numerical features scaled successfully.")
data.head()

# numerical features have been scaled
data = pd.get_dummies(data, drop_first=True)# drop_first=True avoids multicollinearity
print("One-hot encoding applied to categorical features successfully.")
data.head()

#shape of the DataFrame.
print("Updated DataFrame Info:")
data.info()
print("\nUpdated DataFrame Shape:")
print(data.shape)

# target variable
y = data['Churn_Yes'] # Target variable
X = data.drop('Churn_Yes', axis=1) # Features (all columns except 'Churn_Yes')
print("Features (X) and Target (y) separated.")

# Remove customerID columns from the feature set to avoid overfitting
customer_id_cols = [col for col in X.columns if col.startswith('customerID_')]
X = X.drop(columns=customer_id_cols)
print(f"Shape of X after removing customerID columns: {X.shape}")

"""1. Data Splitting Methodologies (Step by Step)"""

# Get the total number of samples in the feature set X
num_samples = X.shape[0]
print(f"Total number of samples in X: {num_samples}")

# Display the distribution and percentage distribution of the target variable 'Churn_Yes'
print("Distribution of target variable (Churn_Yes):")
print(y.value_counts())
print("\nPercentage distribution of target variable (Churn_Yes):")
print(y.value_counts(normalize=True) * 100)

plt.figure(figsize=(6, 6))
plt.pie(y.value_counts(), labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])
plt.title('Distribution of Churn (Churn_Yes)', fontsize=16)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

print("Data split into training and testing sets successfully.")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Split the data into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,# 20% of data for testing
    stratify=y, # Ensure target variable distribution is maintained in splits
    random_state=42# For reproducibility
)

# Split the data into training, validation, and test sets
# First split: Train + Temp (70% train, 30% temp)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42
)

# Second split: Validation + Test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp,
    test_size=0.5,
    random_state=42
)

# Print the shapes of the resulting datasets
print(X_train.shape, X_val.shape, X_test.shape)

# Manually split the data into training and testing sets based on an index
split_index = int(len(data) * 0.8)# 80% for training

train = data.iloc[:split_index]# Training data
test = data.iloc[split_index:] # Testing data

X_train = train.drop("Churn_Yes", axis=1) # Features for training
y_train = train["Churn_Yes"] # Target for training

X_test = test.drop("Churn_Yes", axis=1) # Features for testing
y_test = test["Churn_Yes"] # Target for testing


print("Train data:\n",X_train,y_train)
print("\n")
print("test data:\n" ,X_test,y_test)

# Display the first few rows of the DataFrame to quickly inspect its current state

data.head()

# Define a list of selected domain-specific features
domain_features = [
    "tenure",
    "MonthlyCharges",
    "Contract_One year",
    "Contract_Two year",
    "PaymentMethod_Credit card (automatic)",
    "PaymentMethod_Electronic check",
    "PaymentMethod_Mailed check",
    "InternetService_Fiber optic",
    "InternetService_No"
]

X_domain = X[domain_features] # Create a new DataFrame with only the selected domain features

# Calculate and visualize the correlation matrix for 'tenure', 'MonthlyCharges', and 'Churn_Yes'
correlation = data[["tenure", "MonthlyCharges", "Churn_Yes"]].corr()
sns.heatmap(correlation, annot=True, cmap="coolwarm")# Create a heatmap to show correlations
plt.show()
plt.show()

"""4.2 Chi-Square Test (Categorical Features)"""

# Identify the one-hot encoded categorical columns from X_domain
# These columns are already numerical (0/1) and do not need further encoding for chi2.
categorical_ohe_features = [
    'Contract_One year',
    'Contract_Two year',
    'PaymentMethod_Credit card (automatic)',
    'PaymentMethod_Electronic check',
    'PaymentMethod_Mailed check',
    'InternetService_Fiber optic',
    'InternetService_No'
]

X_cat = X_domain[categorical_ohe_features]

# Perform chi-square test
chi_scores, p_values = chi2(X_cat, y)

# Create a DataFrame to display the chi-square scores and p-values
chi_df = pd.DataFrame({
    "Feature": X_cat.columns,
    "Chi-Square Score": chi_scores,
    "p-value": p_values
})

print(chi_df.sort_values(by="Chi-Square Score", ascending=False))

plt.figure(figsize=(12, 7))
sns.barplot(x='Chi-Square Score', y='Feature', data=chi_df.sort_values(by='Chi-Square Score', ascending=False), palette='viridis')
plt.title('Chi-Square Scores for Categorical Features (Importance to Churn)', fontsize=16)
plt.xlabel('Chi-Square Score', fontsize=12)
plt.ylabel('Categorical Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""Step 5: Model-Based Feature Importance"""

# Initialize and train a RandomForestClassifier to get feature importances
rf = RandomForestClassifier(random_state=42)
rf.fit(X_domain.select_dtypes(include="number"), y)

importances = rf.feature_importances_# Get feature importances from the trained model

# Create a DataFrame to store feature importance scores
feature_importance = pd.DataFrame({
    "Feature": X_domain.select_dtypes(include="number").columns,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print(feature_importance)# Print the feature importance scores

"""Step 6: Final Selected Features"""

# Define the final list of selected features based on previous analysis
selected_features = [
    "tenure",
    "MonthlyCharges",
    "Contract_One year",
    "Contract_Two year",
    "PaymentMethod_Credit card (automatic)",
    "PaymentMethod_Electronic check",
    "PaymentMethod_Mailed check",
    "InternetService_Fiber optic",
    "InternetService_No"
]

X_selected = X[selected_features] # Create a new DataFrame with only the final selected features
print(X_selected)

X = data.drop("Churn_Yes", axis=1) # Define features (X) by dropping the target variable
y = data["Churn_Yes"] # Define target variable (y)

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,# Allocate 20% of the data for testing
    random_state=42,# Set random state for reproducibility
    stratify=y # Ensure class distribution is preserved across splits
)

# Define a dictionary of machine learning models to be evaluated
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),# Logistic Regression model
    "Decision Tree": DecisionTreeClassifier(random_state=42), # Decision Tree Classifier
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42), # Random Forest Classifier
    "Gradient Boosting": GradientBoostingClassifier(random_state=42)# Gradient Boosting Classifier
}

for name, model in models.items():
    model.fit(X_train, y_train) # Train each model on the training data
    y_pred = model.predict(X_test)# Make predictions on the test data
    acc = accuracy_score(y_test, y_pred) # Calculate accuracy score
    print(f"{name} Accuracy: {acc:.4f}")# Print the accuracy for each model

X_numeric = X.astype(int) # Convert all columns in X to integer type for Random Forest compatibility

# Instantiate and train a RandomForestClassifier model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_numeric, y)

# Get feature importances
importances = rf.feature_importances_

# Create a DataFrame for feature importance
feature_importance = pd.DataFrame({
    "Feature": X_numeric.columns,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

print("Model-Based Feature Importance (Random Forest):")
print(feature_importance.head(10)) # Display top 10 features

plt.figure(figsize=(12, 7))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10), palette='magma', hue='Feature', legend=False)
plt.title('Top 10 Most Important Features (Random Forest)', fontsize=16)
plt.xlabel('Feature Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

data["Churn_Yes"] = data["Churn_Yes"].astype(int) # Convert the 'Churn_Yes' column to integer type (0 or 1)

X = data.drop("Churn_Yes", axis=1)   # Define input features by dropping the target variable
y = data["Churn_Yes"]                 # Define the target variable


X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)
model = LogisticRegression(max_iter=1000)# Instantiate Logistic Regression model
model.fit(X_train, y_train)# Train the Logistic Regression model on the training data

y_pred = model.predict(X_test) # Make predictions on the test set
accuracy = accuracy_score(y_test, y_pred)# Calculate the accuracy of the model
print("Model Accuracy:", accuracy)# Print the calculated accuracy

data_model = DecisionTreeClassifier(random_state=42) # Instantiate a Decision Tree Classifier
data_model.fit(X_train, y_train) # Train the Decision Tree model on the training data

y_pred = model.predict(X_test) # Get class predictions from the model
y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class (churn) for ROC-AUC

accuracy = accuracy_score(y_test, y_pred) # Calculate accuracy
precision = precision_score(y_test, y_pred)# Calculate precision
recall = recall_score(y_test, y_pred) # Calculate recall
f1 = f1_score(y_test, y_pred) # Calculate F1-score
roc_auc = roc_auc_score(y_test, y_prob)# Calculate ROC-AUC score

print("Accuracy :", accuracy)
print("Precision:", precision)
print("Recall   :", recall)
print("F1-score :", f1)
print("ROC-AUC  :", roc_auc)

# Print a detailed classification report
print(classification_report(y_test, y_pred))

# Print the confusion matrix
print(confusion_matrix(y_test, y_pred))

